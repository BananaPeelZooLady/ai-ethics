Respect for people as individuals is essential. Data collection of personal
information and behavior may include less than fully informed consent from the
subjects. Think about how the analysis might affect each subject involved. If
the data represents any vulnerable individuals (elderly, e.g.) or individuals
with diminished autonomy (children, e.g.) special consideration should be given
to their protection. People should have their individuality and autonomy
respected, and steps should be taken to protect them from harm.

* Have you taken appropriate data protection steps considering the sensitivity
  of the data?

* If the project is for a public entity, can you disclose the sources of your
  data?

* >Have you confirmed that your data accurately reflects the real-world
  situation for the problem you are trying to solve? Can you also consider
  alternative data sources?

* >Have you checked the internal consistency of the data (through random
  sampling, for example)? Should you report any issues to stakeholders? How
  will you assess the data for both explicit and implicit biases? Consider
  specific subpopulations too.

* Will the benefits of your design extend to the entire population or could
  there be subgroups who are inadvertently excluded? In instances where there
  is a known risk of discrimination, include details of the application and its
  functions along with samples of the data and details about the source of data.
  Is there a process to take effective action to mitigate discrimination?

* For projects that have a significant risk of causing human rights abuse, is
  it possible to submit them for independent third-party audits?

* >Can stakeholders audit the system? - note:add i.e., probing, understanding, reviewing of system behavior

* >>Do individuals have the ability to understand how and why an automated
  decision has been made?

* Is there a plan to communicate the decisions about trade-offs project
  assumptions, shortcomings, error rates, etc. to stakeholders?

* >Access and correction: Is there a process for people to correct data or
  contest erroneous decisions?

* >If appropriate, is there a way for data to be removed when necessary (e.g.,
  the right to be forgotten as practiced in the EU and Argentina)? Similarly
  can people contest the collection and use of their data?

2.2. Social impact
AI is increasingly becoming a more prominent part of many aspects of our lives,
including decision-making processes that could alter the course of it.
Successful designers view possible social impact not as an afterthought or
hindrance to a project but actively think about how to prevent harm while the
system is being developed.

2.2.1. AI decisions that could bias or alter societal norms

* >Are there any subgroups which may be advantaged or disadvantaged more than others from the system? Consider
  especially the effects of errors in the system. Are any groups likely to be
  harmed more significantly than others? Can the system be changed to include these groups?

* >Think about what services, products and industries, and perhaps jobs might be
  replaced by the deployment of the algorithm/system. How will this impact
  society?

* >When using human data, do the benefits outweigh the risks to those involved?
  Consider the benefits of a project and the potential harm to those affected
  by it. For example, researchers at Stanford created a vision application that
  predicts sexual preference by analyzing images of faces. There are almost no
  benefits from the technology but great potential costs when it comes to
  impartiality and fairness. As The Economist put it, "In parts of the world
  where being gay is socially unacceptable, or illegal, such an algorithm could
  pose a serious threat to safety." (Economist 2017)

* Are the decisions produced by an algorithmic system explainable to the people
  affected by those decisions? (These explanations must be accessible and understandable to the target audience; purely technical descriptions are not
appropriate for the general public. (8th-grade reading level is appropriate))


* Useful exercise: Imagine yourself subject to all possible outcomes of a
  system, are they all equally fair and just from that point-of-view? (Rawls
  1999)

2.3. Environmental impact
Resource consumption of a deployed system itself might be an overlooked aspect
of the design.

* Consider the energy impact of the computational resources necessary for the
  project.  Is there a way to minimize that impact? Can the computation be
  handled differently to use less energy?

* Do the resources provisioned for the project match the requirements, or are
  they too excessive?

* Does the algorithm/system/design encourage unsustainable behavior?

2.4. Physical interaction
Much of the concern related to AI has to do with respecting individuals'
privacy and rights. Risk of physical injury may also be a concern. Broadly
speaking users should be aware of possible risks and developers must think
through potential unintended consequences of interacting with the technology.

* Is there the equivalent of informed consent for the usage?

* Will individuals interacting with the technology understand the capabilities
  and limits of it?

* Does the design minimize potential risk?

* Do the warning signals match the severity of the danger?

2.5. Misuse and malicious intent
An often-overlooked concern is how a technology might be misused. Misuse can
happen by mistake, overconfidence in the technology, or by bad actors trying to
subvert it. In all cases care should be taken to mitigate potential harm and
provide appropriate restrictions to limit misuse.

(Is there a description of the ideal outcome of the design, including how this
outcome looks across technological, cultural, social, institutional and
organizational spheres.

Useful exercise: Draw a timeline for 5-10 years in the future. Place your
ideal outcome at the end of the timeline, and chart out potential obstacle
and consequences leading up to it. Repeat until you have three possible
timelines. Did any of these timelines account for possible negative
consequences?)

* Are there guardrails in place to prevent off-label usage (the intentional or
  accidental misuse of the design)?

* Has the system been adequately secured to prevent manipulation from outside?

* >If physical devices are used, are they adequately secured from hacking or
  subversion?  For example, in 2016 IoT devices including baby monitors were
  hijacked to launch a massive infrastructure attack against large sections of
  the internet (Sanger and Perlroth 2016).

2.6. Post deployment
The responsibility of a design or system doesn't end at handoff or deployment.
Although every effort should be made to prevent unexpected outcomes,
considering a sunset plan before delivering the project to the client can
prevent confusion about how to solve unforeseen consequences.

* Do individuals and groups have access to meaningful remedy and redress? This
  may include, for example, creating clear processes for redress following
  adverse decisions or effects.

* >What will the reporting process and process for recourse be? (for instance contact information)

* Is there a plan for what to do if the project has unintended consequences?
  This may be part of a maintenance plan and should involve post-launch
  monitoring plans.
